{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 9156733,
     "sourceType": "datasetVersion",
     "datasetId": 5531714
    }
   ],
   "dockerImageVersionId": 30746,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install transformers torch numpy pandas scikit-learn matplotlib",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-15T13:04:35.658759Z",
     "iopub.execute_input": "2024-08-15T13:04:35.659767Z",
     "iopub.status.idle": "2024-08-15T13:05:10.666547Z",
     "shell.execute_reply.started": "2024-08-15T13:04:35.659718Z",
     "shell.execute_reply": "2024-08-15T13:05:10.665224Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset,random_split\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-15T13:05:10.668302Z",
     "iopub.execute_input": "2024-08-15T13:05:10.668674Z",
     "iopub.status.idle": "2024-08-15T13:05:10.676076Z",
     "shell.execute_reply.started": "2024-08-15T13:05:10.668639Z",
     "shell.execute_reply": "2024-08-15T13:05:10.674913Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "class AnglePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AnglePredictor, self).__init__()\n",
    "        config = BertConfig(\n",
    "            hidden_size=128,  \n",
    "            num_attention_heads=2,  \n",
    "            num_hidden_layers=2, \n",
    "            intermediate_size=128,  \n",
    "            max_position_embeddings=12246  \n",
    "        )\n",
    "\n",
    "        self.embeddings = nn.Linear(12, 128)  \n",
    "        self.transformer = BertModel(config)\n",
    "        self.fc = nn.Linear(128, 14)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)  \n",
    "        attention_mask = torch.ones(x.size()[:-1], device=x.device)  \n",
    "        x = self.transformer(inputs_embeds=x, attention_mask=attention_mask)[0]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-15T13:05:10.679799Z",
     "iopub.execute_input": "2024-08-15T13:05:10.680166Z",
     "iopub.status.idle": "2024-08-15T13:05:10.691675Z",
     "shell.execute_reply.started": "2024-08-15T13:05:10.680136Z",
     "shell.execute_reply": "2024-08-15T13:05:10.690168Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load data\nX_train = np.load('/kaggle/input/mydata/X_train_padding.npy')\ny_train = np.load('/kaggle/input/mydata/y_train_padding.npy')\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Convert data to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-15T13:05:10.693835Z",
     "iopub.execute_input": "2024-08-15T13:05:10.694334Z",
     "iopub.status.idle": "2024-08-15T13:05:11.323846Z",
     "shell.execute_reply.started": "2024-08-15T13:05:10.694294Z",
     "shell.execute_reply": "2024-08-15T13:05:11.322751Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model = AnglePredictor()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-15T13:05:11.325586Z",
     "iopub.execute_input": "2024-08-15T13:05:11.325971Z",
     "iopub.status.idle": "2024-08-15T13:05:11.447096Z",
     "shell.execute_reply.started": "2024-08-15T13:05:11.325938Z",
     "shell.execute_reply": "2024-08-15T13:05:11.446095Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "num_epochs = 10\ntrain_losses = []\ntest_losses = []",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-15T13:05:11.448709Z",
     "iopub.execute_input": "2024-08-15T13:05:11.449114Z",
     "iopub.status.idle": "2024-08-15T13:05:11.454518Z",
     "shell.execute_reply.started": "2024-08-15T13:05:11.449080Z",
     "shell.execute_reply": "2024-08-15T13:05:11.453221Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from torch.utils.data import DataLoader, TensorDataset\n\n# Create DataLoader\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)  # Batch size of 1 due to sequence length\n\n# Training loop\nepochs = 20\nmodel.train()\nfor epoch in range(epochs):\n    epoch_loss = 0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        print(f'Epoch {epoch}/{epochs}, Loss: {epoch_loss/len(train_loader)}')\n    print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader)}')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-15T13:05:11.456309Z",
     "iopub.execute_input": "2024-08-15T13:05:11.456775Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 0/20, Loss: 8.243240921585649\nEpoch 0/20, Loss: 26.15350115740741\nEpoch 0/20, Loss: 36.12481840157214\nEpoch 0/20, Loss: 46.4441362545814\nEpoch 0/20, Loss: 52.8715503833912\nEpoch 0/20, Loss: 65.29637843002507\nEpoch 0/20, Loss: 84.97649694372106\nEpoch 0/20, Loss: 102.15033863208912\nEpoch 0/20, Loss: 112.7909696602527\nEpoch 0/20, Loss: 124.39739161361882\nEpoch 0/20, Loss: 138.38001693913967\nEpoch 0/20, Loss: 145.40054811077354\nEpoch 0/20, Loss: 159.68886763961225\nEpoch 0/20, Loss: 169.14690709997106\nEpoch 0/20, Loss: 177.4495691370081\nEpoch 0/20, Loss: 183.31481707537617\nEpoch 0/20, Loss: 193.46425148292823\nEpoch 0/20, Loss: 204.47840711805554\nEpoch 0/20, Loss: 215.29619532455632\nEpoch 0/20, Loss: 224.06606415171683\nEpoch 0/20, Loss: 230.89666521990742\nEpoch 0/20, Loss: 240.88370542173033\nEpoch 0/20, Loss: 250.7502803096065\nEpoch 0/20, Loss: 261.7063372576678\nEpoch 0/20, Loss: 267.17965924298323\nEpoch 0/20, Loss: 274.91635357892073\nEpoch 0/20, Loss: 283.88735057689524\nEpoch 0/20, Loss: 296.2240898226514\nEpoch 0/20, Loss: 304.2152163658613\nEpoch 0/20, Loss: 313.02294582790796\nEpoch 0/20, Loss: 319.95363098898054\nEpoch 0/20, Loss: 331.93207879714026\nEpoch 0/20, Loss: 345.1135762532552\nEpoch 0/20, Loss: 354.9337312674817\nEpoch 0/20, Loss: 362.02914880823204\nEpoch 0/20, Loss: 368.5764608500916\nEpoch 0/20, Loss: 377.03348079728494\nEpoch 0/20, Loss: 387.0398329746576\nEpoch 0/20, Loss: 396.60353860737365\nEpoch 0/20, Loss: 403.19172235182776\nEpoch 0/20, Loss: 410.3683731644242\nEpoch 0/20, Loss: 425.7002145272714\nEpoch 0/20, Loss: 434.91532577703026\nEpoch 0/20, Loss: 443.0821310914593\nEpoch 0/20, Loss: 451.88319114402486\nEpoch 0/20, Loss: 461.05178079487365\nEpoch 0/20, Loss: 476.89277441707657\nEpoch 0/20, Loss: 484.6672660921827\nEpoch 0/20, Loss: 493.21175017180263\nEpoch 0/20, Loss: 500.0414515365789\nEpoch 0/20, Loss: 509.1004717791522\nEpoch 0/20, Loss: 517.162924307364\nEpoch 0/20, Loss: 535.5588013448832\nEpoch 0/20, Loss: 541.6389759205006\nEpoch 0/20, Loss: 550.5287019706067\nEpoch 0/20, Loss: 569.4275765124662\nEpoch 0/20, Loss: 578.7773365915557\nEpoch 0/20, Loss: 598.7780735345534\nEpoch 0/20, Loss: 608.954643720462\nEpoch 0/20, Loss: 615.8370998052903\nEpoch 0/20, Loss: 622.3613194595149\nEpoch 0/20, Loss: 638.6318091875241\nEpoch 0/20, Loss: 647.4010375223043\nEpoch 0/20, Loss: 657.3816562228733\nEpoch 0/20, Loss: 673.1349807550878\nEpoch 0/20, Loss: 679.8011862672405\nEpoch 0/20, Loss: 700.1089179898486\nEpoch 0/20, Loss: 708.6760913236642\nEpoch 0/20, Loss: 716.6874016655815\nEpoch 0/20, Loss: 776.3327527458285\nEpoch 0/20, Loss: 787.0882466634115\nEpoch 0/20, Loss: 796.3239335425106\nEpoch 0/20, Loss: 815.261025134428\nEpoch 0/20, Loss: 825.8228409378617\nEpoch 0/20, Loss: 832.7830332061391\nEpoch 0/20, Loss: 837.9024518801842\nEpoch 0/20, Loss: 856.7971654821325\nEpoch 0/20, Loss: 863.584504680869\nEpoch 0/20, Loss: 879.6392675329138\nEpoch 0/20, Loss: 888.0259930645979\nEpoch 0/20, Loss: 908.8741285536024\nEpoch 0/20, Loss: 917.0476952070071\nEpoch 0/20, Loss: 925.1331828553\nEpoch 0/20, Loss: 934.331130416305\nEpoch 0/20, Loss: 939.9722979510271\nEpoch 0/20, Loss: 950.448969711492\nEpoch 0/20, Loss: 958.2836563675492\nEpoch 0/20, Loss: 968.7886552221981\nEpoch 0/20, Loss: 978.2633602942949\nEpoch 0/20, Loss: 987.4124284909095\nEpoch 0/20, Loss: 996.3186310191213\nEpoch 0/20, Loss: 1003.5535741735388\nEpoch 0/20, Loss: 1011.3615394874855\nEpoch 0/20, Loss: 1022.0960018958574\nEpoch 0/20, Loss: 1032.6161781593605\nEpoch 0/20, Loss: 1041.1129862467449\nEpoch 0/20, Loss: 1050.9132355113088\nEpoch 0/20, Loss: 1057.6089368278597\nEpoch 0/20, Loss: 1072.8273040394724\nEpoch 0/20, Loss: 1081.4651899926457\nEpoch 0/20, Loss: 1092.5920768078463\nEpoch 0/20, Loss: 1103.001001805435\nEpoch 0/20, Loss: 1112.6151277518566\nEpoch 0/20, Loss: 1123.8235869231048\nEpoch 0/20, Loss: 1131.9923009048273\nEpoch 0/20, Loss: 1136.854854254075\nEpoch 0/20, Loss: 1151.7436165515287\nEpoch 0/20, Loss: 1156.7369354624807\nEpoch 0/20, Loss: 1164.6099047248747\nEpoch 0/20, Loss: 1174.6575324918017\nEpoch 0/20, Loss: 1181.4836041485821\nEpoch 0/20, Loss: 1193.797128936391\nEpoch 0/20, Loss: 1198.286450798129\nEpoch 0/20, Loss: 1205.2470529815298\nEpoch 0/20, Loss: 1212.996108820409\nEpoch 0/20, Loss: 1221.0633431893807\nEpoch 0/20, Loss: 1229.2082074954185\nEpoch 0/20, Loss: 1234.9855380588108\nEpoch 0/20, Loss: 1243.7925546905142\nEpoch 0/20, Loss: 1248.7341466833043\nEpoch 0/20, Loss: 1254.2061820324557\nEpoch 0/20, Loss: 1263.3510832609954\nEpoch 0/20, Loss: 1272.0199788411458\nEpoch 0/20, Loss: 1281.590970263069\nEpoch 0/20, Loss: 1291.6078001422648\nEpoch 0/20, Loss: 1300.8817455150463\nEpoch 0/20, Loss: 1313.067542558835\nEpoch 0/20, Loss: 1318.5979791335117\nEpoch 0/20, Loss: 1334.5691479341483\nEpoch 0/20, Loss: 1343.6582750862028\nEpoch 0/20, Loss: 1351.2870351532358\nEpoch 0/20, Loss: 1364.6632890112605\nEpoch 0/20, Loss: 1372.3984363697193\nEpoch 0/20, Loss: 1381.761824619623\nEpoch 0/20, Loss: 1387.2134154519918\nEpoch 0/20, Loss: 1392.09713443709\nEpoch 0/20, Loss: 1398.7186750247154\nEpoch 0/20, Loss: 1406.2568559057918\nEpoch 0/20, Loss: 1415.5498638388551\nEpoch 0/20, Loss: 1431.9252586835696\nEpoch 0/20, Loss: 1443.1525596336082\nEpoch 0/20, Loss: 1451.7779265980662\nEpoch 0/20, Loss: 1460.7675385651764\nEpoch 0/20, Loss: 1470.2013176812065\nEpoch 0/20, Loss: 1475.7846438560957\nEpoch 0/20, Loss: 1481.0457251277971\nEpoch 0/20, Loss: 1487.8315218701775\nEpoch 0/20, Loss: 1494.8089735243057\nEpoch 0/20, Loss: 1503.5272488064236\nEpoch 0/20, Loss: 1512.5534359025366\nEpoch 0/20, Loss: 1528.2988138081114\nEpoch 0/20, Loss: 1533.921110553506\nEpoch 0/20, Loss: 1554.0913082169898\nEpoch 0/20, Loss: 1569.0729313603154\nEpoch 0/20, Loss: 1576.2176155749662\nEpoch 0/20, Loss: 1586.9400615927614\nEpoch 0/20, Loss: 1594.9690231511622\nEpoch 0/20, Loss: 1603.6863286524642\nEpoch 1/20, Loss: 1603.6863286524642\nEpoch 1/20, Loss: 16.100138346354168\nEpoch 1/20, Loss: 24.102452407648535\nEpoch 1/20, Loss: 31.137115101755402\nEpoch 1/20, Loss: 36.806362952715084\nEpoch 1/20, Loss: 52.38329739040799\nEpoch 1/20, Loss: 70.75012621467496\nEpoch 1/20, Loss: 76.83114021207079\nEpoch 1/20, Loss: 81.98888444311825\nEpoch 1/20, Loss: 89.58229083779418\nEpoch 1/20, Loss: 98.47550342701099\nEpoch 1/20, Loss: 118.78142067238137\nEpoch 1/20, Loss: 126.28228646737558\nEpoch 1/20, Loss: 133.59995411060476\nEpoch 1/20, Loss: 141.25645578643423\nEpoch 1/20, Loss: 147.7757074803482\nEpoch 1/20, Loss: 156.32921195324556\nEpoch 1/20, Loss: 165.15564228575906\nEpoch 1/20, Loss: 170.5666409716194\nEpoch 1/20, Loss: 177.984809404538\nEpoch 1/20, Loss: 185.77121951256268\nEpoch 1/20, Loss: 194.37992312584393\nEpoch 1/20, Loss: 200.54399052372685\nEpoch 1/20, Loss: 207.75371485580632\nEpoch 1/20, Loss: 216.2035688235436\nEpoch 1/20, Loss: 225.09368218315973\nEpoch 1/20, Loss: 233.3211873372396\nEpoch 1/20, Loss: 239.80754635657794\nEpoch 1/20, Loss: 250.6769115306713\nEpoch 1/20, Loss: 260.7959971486786\nEpoch 1/20, Loss: 272.8760737666377\nEpoch 1/20, Loss: 281.7107965916763\nEpoch 1/20, Loss: 288.3365832670235\nEpoch 1/20, Loss: 295.9431988751447\nEpoch 1/20, Loss: 306.14795524691357\nEpoch 1/20, Loss: 314.52002932701583\nEpoch 1/20, Loss: 319.6607375910253\nEpoch 1/20, Loss: 329.52930554048515\nEpoch 1/20, Loss: 359.40251366886093\nEpoch 1/20, Loss: 371.35758425865646\nEpoch 1/20, Loss: 379.93537469557776\nEpoch 1/20, Loss: 390.770432083695\nEpoch 1/20, Loss: 399.88282644012827\nEpoch 1/20, Loss: 409.4256226339458\nEpoch 1/20, Loss: 418.6740025649836\nEpoch 1/20, Loss: 434.12390852563175\nEpoch 1/20, Loss: 441.9565893102575\nEpoch 1/20, Loss: 450.3847192834925\nEpoch 1/20, Loss: 458.75969366379724\nEpoch 1/20, Loss: 463.4899167661314\nEpoch 1/20, Loss: 472.20587421935284\nEpoch 1/20, Loss: 482.6618128647039\nEpoch 1/20, Loss: 497.0633247281298\nEpoch 1/20, Loss: 505.9114994001977\nEpoch 1/20, Loss: 512.4585394211757\nEpoch 1/20, Loss: 520.0765866880064\nEpoch 1/20, Loss: 526.3680065767264\nEpoch 1/20, Loss: 534.7500361689815\nEpoch 1/20, Loss: 547.7059341242284\nEpoch 1/20, Loss: 557.7716380931713\nEpoch 1/20, Loss: 563.7529085889274\nEpoch 1/20, Loss: 571.6612857771509\nEpoch 1/20, Loss: 578.1670095184703\nEpoch 1/20, Loss: 592.2716614523051\nEpoch 1/20, Loss: 607.1089959792149\nEpoch 1/20, Loss: 613.4576212565104\nEpoch 1/20, Loss: 628.7223917643229\nEpoch 1/20, Loss: 646.690451539593\nEpoch 1/20, Loss: 655.2610895604263\nEpoch 1/20, Loss: 659.8411706995081\nEpoch 1/20, Loss: 665.263783019266\nEpoch 1/20, Loss: 674.7980395658517\nEpoch 1/20, Loss: 685.2915690857687\nEpoch 1/20, Loss: 692.7215436770592\nEpoch 1/20, Loss: 697.4664615584009\nEpoch 1/20, Loss: 704.4567539544753\nEpoch 1/20, Loss: 713.1336865837192\nEpoch 1/20, Loss: 720.5323734989872\nEpoch 1/20, Loss: 731.3411006221065\nEpoch 1/20, Loss: 736.7351055380739\nEpoch 1/20, Loss: 745.3005359790943\nEpoch 1/20, Loss: 751.6796381444107\nEpoch 1/20, Loss: 759.5404327769338\nEpoch 1/20, Loss: 774.1372918023003\nEpoch 1/20, Loss: 789.2414855957031\nEpoch 1/20, Loss: 793.8789258415317\nEpoch 1/20, Loss: 802.0857559015722\nEpoch 1/20, Loss: 812.0208860797646\nEpoch 1/20, Loss: 826.4288661627122\nEpoch 1/20, Loss: 836.1999790521315\nEpoch 1/20, Loss: 854.8402077003761\nEpoch 1/20, Loss: 860.6373509536555\nEpoch 1/20, Loss: 872.9410905249325\nEpoch 1/20, Loss: 882.8181061921297\nEpoch 1/20, Loss: 891.5821261935764\nEpoch 1/20, Loss: 897.460506486304\nEpoch 1/20, Loss: 906.1161190080054\nEpoch 1/20, Loss: 914.481896671248\nEpoch 1/20, Loss: 928.8870616018037\nEpoch 1/20, Loss: 936.975247606819\nEpoch 1/20, Loss: 956.4096717363523\nEpoch 1/20, Loss: 963.5723929699557\nEpoch 1/20, Loss: 974.2134633005401\nEpoch 1/20, Loss: 991.1557722680362\nEpoch 1/20, Loss: 998.9339983904803\nEpoch 1/20, Loss: 1006.5043191792053\nEpoch 1/20, Loss: 1017.5909642349055\nEpoch 1/20, Loss: 1027.0960731035398\nEpoch 1/20, Loss: 1033.3449748474875\nEpoch 1/20, Loss: 1042.237875479239\nEpoch 1/20, Loss: 1050.438868769893\nEpoch 1/20, Loss: 1060.5309557502653\nEpoch 1/20, Loss: 1067.0434672037761\nEpoch 1/20, Loss: 1076.0189303174432\nEpoch 1/20, Loss: 1084.8940938313801\nEpoch 1/20, Loss: 1092.8076175642602\nEpoch 1/20, Loss: 1104.3144738468122\nEpoch 1/20, Loss: 1111.5019730932918\nEpoch 1/20, Loss: 1116.1232311107494\nEpoch 1/20, Loss: 1128.78205815068\nEpoch 1/20, Loss: 1136.8370041081935\nEpoch 1/20, Loss: 1145.3594936794705\nEpoch 1/20, Loss: 1154.4053702648775\nEpoch 1/20, Loss: 1162.9023863239054\nEpoch 1/20, Loss: 1170.452986728998\nEpoch 1/20, Loss: 1175.7237496081693\nEpoch 1/20, Loss: 1185.7265203028549\nEpoch 1/20, Loss: 1202.8050657672648\nEpoch 1/20, Loss: 1212.416466983748\nEpoch 1/20, Loss: 1219.541045765818\nEpoch 1/20, Loss: 1228.6960607458043\nEpoch 1/20, Loss: 1235.0144468707804\nEpoch 1/20, Loss: 1243.9764483416523\nEpoch 1/20, Loss: 1249.9945976351514\nEpoch 1/20, Loss: 1258.9107006685233\nEpoch 1/20, Loss: 1267.4219341513551\nEpoch 1/20, Loss: 1276.1980443883824\nEpoch 1/20, Loss: 1282.6271253044222\nEpoch 1/20, Loss: 1291.2693990071614\nEpoch 1/20, Loss: 1296.2758144802517\nEpoch 1/20, Loss: 1304.750784791546\nEpoch 1/20, Loss: 1311.9361606174045\nEpoch 1/20, Loss: 1318.5304945957514\nEpoch 1/20, Loss: 1337.748085681303\nEpoch 1/20, Loss: 1345.1084651240596\nEpoch 1/20, Loss: 1353.291263156467\nEpoch 1/20, Loss: 1362.6172247992622\nEpoch 1/20, Loss: 1379.8157744231048\nEpoch 1/20, Loss: 1385.7999064127605\nEpoch 1/20, Loss: 1394.984871569975\nEpoch 1/20, Loss: 1400.042290204837\nEpoch 1/20, Loss: 1409.9038519211758\nEpoch 1/20, Loss: 1415.6473972650222\nEpoch 1/20, Loss: 1434.315771409023\nEpoch 1/20, Loss: 1439.3128228835117\nEpoch 1/20, Loss: 1454.6343003261236\nEpoch 1/20, Loss: 1469.466988645954\nEpoch 1/20, Loss: 1473.5149408034335\nEpoch 1/20, Loss: 1480.9493491090375\nEpoch 1/20, Loss: 1485.4539090380256\nEpoch 2/20, Loss: 1485.4539090380256\nEpoch 2/20, Loss: 9.03442985628858\nEpoch 2/20, Loss: 27.17382511091821\nEpoch 2/20, Loss: 31.63951318057967\nEpoch 2/20, Loss: 39.86952002254533\nEpoch 2/20, Loss: 47.40991926781925\nEpoch 2/20, Loss: 56.165381537543404\nEpoch 2/20, Loss: 64.62901965482736\nEpoch 2/20, Loss: 71.28223748854649\nEpoch 2/20, Loss: 82.64462657033661\nEpoch 2/20, Loss: 92.80371131426023\nEpoch 2/20, Loss: 101.14885005244503\nEpoch 2/20, Loss: 105.88796544958043\nEpoch 2/20, Loss: 110.80650386103878\nEpoch 2/20, Loss: 115.1540986991223\nEpoch 2/20, Loss: 124.42250871069638\nEpoch 2/20, Loss: 132.26765347704475\nEpoch 2/20, Loss: 141.07658028308256\nEpoch 2/20, Loss: 148.05759382836612\nEpoch 2/20, Loss: 155.165613245081\nEpoch 2/20, Loss: 163.6316279658565\nEpoch 2/20, Loss: 170.849824881848\nEpoch 2/20, Loss: 181.65129861713928\nEpoch 2/20, Loss: 195.3701058846933\nEpoch 2/20, Loss: 203.59289927541474\nEpoch 2/20, Loss: 211.25824728129822\nEpoch 2/20, Loss: 217.18833716121722\nEpoch 2/20, Loss: 225.23314073350696\nEpoch 2/20, Loss: 235.56640323591822\nEpoch 2/20, Loss: 243.75760603539737\nEpoch 2/20, Loss: 251.82771357783565\nEpoch 2/20, Loss: 265.1850781852816\nEpoch 2/20, Loss: 271.2049835581838\nEpoch 2/20, Loss: 280.4998368628231\nEpoch 2/20, Loss: 286.9608323838976\nEpoch 2/20, Loss: 292.019100236304\nEpoch 2/20, Loss: 297.20134970582563\nEpoch 2/20, Loss: 305.0137427059221\nEpoch 2/20, Loss: 313.2033020773052\nEpoch 2/20, Loss: 318.09099663628473\nEpoch 2/20, Loss: 326.15480550130206\nEpoch 2/20, Loss: 344.90153070143714\nEpoch 2/20, Loss: 352.6556306061921\nEpoch 2/20, Loss: 362.7042002736786\nEpoch 2/20, Loss: 371.2803668740355\nEpoch 2/20, Loss: 377.25751222210164\nEpoch 2/20, Loss: 382.6603909716194\nEpoch 2/20, Loss: 396.4783283751688\nEpoch 2/20, Loss: 406.00317721896704\nEpoch 2/20, Loss: 415.4596516173563\nEpoch 2/20, Loss: 423.3414423436294\nEpoch 2/20, Loss: 428.91812435197244\nEpoch 2/20, Loss: 436.1348066918644\nEpoch 2/20, Loss: 445.4559337474682\nEpoch 2/20, Loss: 453.68662403247976\nEpoch 2/20, Loss: 459.2466204607928\nEpoch 2/20, Loss: 466.0462736906829\nEpoch 2/20, Loss: 471.8410550341194\nEpoch 2/20, Loss: 479.51727558654034\nEpoch 2/20, Loss: 486.7673313470534\nEpoch 2/20, Loss: 494.72783331223474\nEpoch 2/20, Loss: 509.0435538586275\nEpoch 2/20, Loss: 521.2995119448061\nEpoch 2/20, Loss: 537.4838531870901\nEpoch 2/20, Loss: 552.3944736056858\nEpoch 2/20, Loss: 560.9010601279176\nEpoch 2/20, Loss: 566.8021326889226\nEpoch 2/20, Loss: 572.7175127194251\nEpoch 2/20, Loss: 579.4986632547261\nEpoch 2/20, Loss: 585.7750748999325\nEpoch 2/20, Loss: 593.7903434847608\nEpoch 2/20, Loss: 598.6749007613571\nEpoch 2/20, Loss: 615.5453042866271\nEpoch 2/20, Loss: 621.6291903272087\nEpoch 2/20, Loss: 635.9813993477527\nEpoch 2/20, Loss: 640.3326886965905\nEpoch 2/20, Loss: 646.3876335238233\nEpoch 2/20, Loss: 655.1737482518325\nEpoch 2/20, Loss: 673.9624144000771\nEpoch 2/20, Loss: 678.3410814073351\nEpoch 2/20, Loss: 683.9109587492766\nEpoch 2/20, Loss: 688.8037576557678\nEpoch 2/20, Loss: 703.3772635754244\nEpoch 2/20, Loss: 717.054012345679\nEpoch 2/20, Loss: 723.7653273594233\nEpoch 2/20, Loss: 728.7356254671827\nEpoch 2/20, Loss: 733.7070926619165\nEpoch 2/20, Loss: 747.1882802704234\nEpoch 2/20, Loss: 754.0483387134693\nEpoch 2/20, Loss: 762.1008342224875\nEpoch 2/20, Loss: 770.6020873269918\nEpoch 2/20, Loss: 778.0720768681279\nEpoch 2/20, Loss: 785.2705624427324\nEpoch 2/20, Loss: 794.5842876669801\nEpoch 2/20, Loss: 811.0687274697386\nEpoch 2/20, Loss: 817.037439416956\nEpoch 2/20, Loss: 820.8856958459925\nEpoch 2/20, Loss: 826.4932194462529\nEpoch 2/20, Loss: 834.5471782919801\nEpoch 2/20, Loss: 841.564599684727\nEpoch 2/20, Loss: 851.074961344401\nEpoch 2/20, Loss: 858.8879383228443\nEpoch 2/20, Loss: 865.3817843213493\nEpoch 2/20, Loss: 872.9856970516252\nEpoch 2/20, Loss: 884.4738238299334\nEpoch 2/20, Loss: 889.8473469298563\nEpoch 2/20, Loss: 898.0438884217062\nEpoch 2/20, Loss: 908.0184766981337\nEpoch 2/20, Loss: 916.8655293782552\nEpoch 2/20, Loss: 926.3997211220824\nEpoch 2/20, Loss: 931.7278351960359\nEpoch 2/20, Loss: 938.5766729660976\nEpoch 2/20, Loss: 947.7094131281347\nEpoch 2/20, Loss: 954.2563363534433\nEpoch 2/20, Loss: 968.3539172513986\nEpoch 2/20, Loss: 975.1898826316551\nEpoch 2/20, Loss: 993.4880627290702\nEpoch 2/20, Loss: 1000.7053064416956\nEpoch 2/20, Loss: 1010.0824675383391\nEpoch 2/20, Loss: 1020.8640875168788\nEpoch 2/20, Loss: 1032.7578757957176\nEpoch 2/20, Loss: 1040.143022702064\nEpoch 2/20, Loss: 1050.9342146508488\nEpoch 2/20, Loss: 1057.6854346004534\nEpoch 2/20, Loss: 1064.3606914002219\nEpoch 2/20, Loss: 1072.2526463638117\nEpoch 2/20, Loss: 1079.2031777464313\nEpoch 2/20, Loss: 1083.9204237196182\nEpoch 2/20, Loss: 1097.828424901138\nEpoch 2/20, Loss: 1105.3171703197338\nEpoch 2/20, Loss: 1121.334199881848\nEpoch 2/20, Loss: 1128.8569803120176\nEpoch 2/20, Loss: 1136.834710015191\nEpoch 2/20, Loss: 1146.2550749602142\nEpoch 2/20, Loss: 1153.2506005557966\nEpoch 2/20, Loss: 1167.0500209478685\nEpoch 2/20, Loss: 1176.1699354383682\nEpoch 2/20, Loss: 1184.3903115354938\nEpoch 2/20, Loss: 1190.0070691520784\nEpoch 2/20, Loss: 1197.4199674629872\nEpoch 2/20, Loss: 1201.910326545621\nEpoch 2/20, Loss: 1209.2519146954571\nEpoch 2/20, Loss: 1222.6875972041378\nEpoch 2/20, Loss: 1228.3775563181182\nEpoch 2/20, Loss: 1234.1728018301505\nEpoch 2/20, Loss: 1239.8116752718702\nEpoch 2/20, Loss: 1248.4419039502557\nEpoch 2/20, Loss: 1264.7760299870997\nEpoch 2/20, Loss: 1271.1239861382378\nEpoch 2/20, Loss: 1275.2637076672213\nEpoch 2/20, Loss: 1283.5110108175395\nEpoch 2/20, Loss: 1292.389895968967\nEpoch 2/20, Loss: 1296.5573210539642\nEpoch 2/20, Loss: 1314.1310138466918\nEpoch 2/20, Loss: 1321.6572054639275\nEpoch 2/20, Loss: 1329.829029224537\nEpoch 2/20, Loss: 1337.0669947494696\nEpoch 2/20, Loss: 1343.7562783323688\nEpoch 2/20, Loss: 1350.8358644085165\nEpoch 2/20, Loss: 1357.9110438970872\nEpoch 2/20, Loss: 1365.606371467496\nEpoch 2/20, Loss: 1374.0387332115645\nEpoch 2/20, Loss: 1383.2528663917824\nEpoch 3/20, Loss: 1383.2528663917824\nEpoch 3/20, Loss: 9.392188554928627\nEpoch 3/20, Loss: 15.00441261574074\nEpoch 3/20, Loss: 24.30399200062693\nEpoch 3/20, Loss: 29.168047116126544\nEpoch 3/20, Loss: 34.3455034420814\nEpoch 3/20, Loss: 41.680740921585645\nEpoch 3/20, Loss: 52.239982699170525\nEpoch 3/20, Loss: 58.61387050298997\nEpoch 3/20, Loss: 67.72042281539352\nEpoch 3/20, Loss: 74.92529372227045\nEpoch 3/20, Loss: 88.67368269555362\nEpoch 3/20, Loss: 105.60543484157986\nEpoch 3/20, Loss: 113.17870189525463\nEpoch 3/20, Loss: 119.76875663097994\nEpoch 3/20, Loss: 126.41139051649306\nEpoch 3/20, Loss: 139.525232385706\nEpoch 3/20, Loss: 147.7595260054977\nEpoch 3/20, Loss: 153.26996527777777\nEpoch 3/20, Loss: 157.83633083767361\nEpoch 3/20, Loss: 221.48150069625288\nEpoch 3/20, Loss: 236.79177931797358\nEpoch 3/20, Loss: 242.41782671139563\nEpoch 3/20, Loss: 250.45127246997976\nEpoch 3/20, Loss: 257.729581479673\nEpoch 3/20, Loss: 270.6821970998505\nEpoch 3/20, Loss: 288.6399909125434\nEpoch 3/20, Loss: 294.31991238064234\nEpoch 3/20, Loss: 303.6048410674672\nEpoch 3/20, Loss: 308.2917476701148\nEpoch 3/20, Loss: 315.57037617247784\nEpoch 3/20, Loss: 379.74370395990064\nEpoch 3/20, Loss: 386.1351224169319\nEpoch 3/20, Loss: 393.2831334243586\nEpoch 3/20, Loss: 400.74469257872784\nEpoch 3/20, Loss: 405.7692200460552\nEpoch 3/20, Loss: 413.6850141360436\nEpoch 3/20, Loss: 419.1950875741464\nEpoch 3/20, Loss: 424.7934163411458\nEpoch 3/20, Loss: 428.7247231565876\nEpoch 3/20, Loss: 434.2170805754485\nEpoch 3/20, Loss: 442.30358020170235\nEpoch 3/20, Loss: 449.00890924901137\nEpoch 3/20, Loss: 455.90453084309894\nEpoch 3/20, Loss: 463.2330386314863\nEpoch 3/20, Loss: 480.33696718569155\nEpoch 3/20, Loss: 493.66140294958046\nEpoch 3/20, Loss: 500.7720642089844\nEpoch 3/20, Loss: 516.1573181152344\nEpoch 3/20, Loss: 522.4557367018712\nEpoch 3/20, Loss: 548.0274612991898\nEpoch 3/20, Loss: 556.9514175226659\nEpoch 3/20, Loss: 563.0073046272183\nEpoch 3/20, Loss: 570.9472528151524\nEpoch 3/20, Loss: 574.998361846547\nEpoch 3/20, Loss: 590.5513162495178\nEpoch 3/20, Loss: 596.6601445704331\nEpoch 3/20, Loss: 607.5106581699702\nEpoch 3/20, Loss: 614.7456932538821\nEpoch 3/20, Loss: 620.0602315266927\nEpoch 3/20, Loss: 628.0446555055217\nEpoch 3/20, Loss: 636.1398183563609\nEpoch 3/20, Loss: 640.1061902458285\nEpoch 3/20, Loss: 646.232418484158\nEpoch 3/20, Loss: 650.5810106065538\nEpoch 3/20, Loss: 664.3510210955584\nEpoch 3/20, Loss: 673.8785886411314\nEpoch 3/20, Loss: 679.4951883951823\nEpoch 3/20, Loss: 692.7447016209732\nEpoch 3/20, Loss: 700.9354410053771\nEpoch 3/20, Loss: 709.2969077781395\nEpoch 3/20, Loss: 718.0935303487895\nEpoch 3/20, Loss: 726.8250675907841\nEpoch 3/20, Loss: 732.108620726032\nEpoch 3/20, Loss: 738.5559541678723\nEpoch 3/20, Loss: 752.0040772991416\nEpoch 3/20, Loss: 759.8550422574267\nEpoch 3/20, Loss: 766.2124452944155\nEpoch 3/20, Loss: 773.6614545657311\nEpoch 3/20, Loss: 778.196511351032\nEpoch 3/20, Loss: 783.2518905828028\nEpoch 3/20, Loss: 798.8142820758584\nEpoch 3/20, Loss: 807.473459502797\nEpoch 3/20, Loss: 812.4939190899885\nEpoch 3/20, Loss: 825.5593713831018\nEpoch 3/20, Loss: 833.6903347439236\nEpoch 3/20, Loss: 841.2214430820795\nEpoch 3/20, Loss: 847.3317283347801\nEpoch 3/20, Loss: 851.5375558358652\nEpoch 3/20, Loss: 855.4694074465905\nEpoch 3/20, Loss: 862.7782046471114\nEpoch 3/20, Loss: 866.7103422188464\nEpoch 3/20, Loss: 872.0777196059993\nEpoch 3/20, Loss: 878.3415580090182\nEpoch 3/20, Loss: 893.8036905924479\nEpoch 3/20, Loss: 901.5555947386189\nEpoch 3/20, Loss: 908.6980990186149\nEpoch 3/20, Loss: 915.4078655478395\nEpoch 3/20, Loss: 923.5342309268905\nEpoch 3/20, Loss: 932.039321711034\nEpoch 3/20, Loss: 941.8345578040605\nEpoch 3/20, Loss: 948.2222222222222\nEpoch 3/20, Loss: 951.7166465476707\nEpoch 3/20, Loss: 958.8758646647135\nEpoch 3/20, Loss: 965.4375372992622\nEpoch 3/20, Loss: 971.9989446946132\nEpoch 3/20, Loss: 981.952368842231\nEpoch 3/20, Loss: 989.0505646128713\nEpoch 3/20, Loss: 993.6652082278405\nEpoch 3/20, Loss: 1000.8582820185909\nEpoch 3/20, Loss: 1008.0732583881896\nEpoch 3/20, Loss: 1014.835717095269\nEpoch 3/20, Loss: 1021.1389043360581\nEpoch 3/20, Loss: 1026.0839531039014\nEpoch 3/20, Loss: 1033.2392890835986\nEpoch 3/20, Loss: 1039.9566533594955\nEpoch 3/20, Loss: 1046.712038770134\nEpoch 3/20, Loss: 1053.3708891691986\nEpoch 3/20, Loss: 1062.1044850290557\nEpoch 3/20, Loss: 1069.086954375844\nEpoch 3/20, Loss: 1073.4534041793258\nEpoch 3/20, Loss: 1080.8513797712915\nEpoch 3/20, Loss: 1093.081748303072\nEpoch 3/20, Loss: 1097.3895184552227\nEpoch 3/20, Loss: 1103.08541568709\nEpoch 3/20, Loss: 1120.0870869954426\nEpoch 3/20, Loss: 1125.6710992506992\nEpoch 3/20, Loss: 1133.0274368097753\nEpoch 3/20, Loss: 1143.047912220896\nEpoch 3/20, Loss: 1148.3161353593991\nEpoch 3/20, Loss: 1157.0788758831259\nEpoch 3/20, Loss: 1162.0621322820216\nEpoch 3/20, Loss: 1169.711417492525\nEpoch 3/20, Loss: 1175.7801121841242\nEpoch 3/20, Loss: 1182.032918671031\nEpoch 3/20, Loss: 1188.2789253894193\nEpoch 3/20, Loss: 1196.4902675298997\nEpoch 3/20, Loss: 1204.8127870912906\nEpoch 3/20, Loss: 1211.8990712106963\nEpoch 3/20, Loss: 1218.8611887237173\nEpoch 3/20, Loss: 1225.6085431134259\nEpoch 3/20, Loss: 1230.0217386881511\nEpoch 3/20, Loss: 1236.7828372908227\nEpoch 3/20, Loss: 1243.3542680151668\nEpoch 3/20, Loss: 1247.378584496769\nEpoch 3/20, Loss: 1251.606150686005\nEpoch 3/20, Loss: 1257.936026867525\nEpoch 3/20, Loss: 1269.091995804398\nEpoch 3/20, Loss: 1277.5546875\nEpoch 3/20, Loss: 1282.5417122546537\nEpoch 4/20, Loss: 1282.5417122546537\nEpoch 4/20, Loss: 6.720734019338349\nEpoch 4/20, Loss: 19.48007767288773\nEpoch 4/20, Loss: 24.692883526837385\nEpoch 4/20, Loss: 37.67487589518229\nEpoch 4/20, Loss: 42.996243323808834\nEpoch 4/20, Loss: 48.317599826388886\nEpoch 4/20, Loss: 55.63898383246528\nEpoch 4/20, Loss: 63.9563892505787\nEpoch 4/20, Loss: 74.8254522629726\nEpoch 4/20, Loss: 78.82233344184027\nEpoch 4/20, Loss: 85.23289809992283\nEpoch 4/20, Loss: 101.82453221450618\nEpoch 4/20, Loss: 109.71903935185185\nEpoch 4/20, Loss: 116.96194043571566\nEpoch 4/20, Loss: 121.7294620466821\nEpoch 4/20, Loss: 129.96071596498842\nEpoch 4/20, Loss: 134.31806248794368\nEpoch 4/20, Loss: 140.5666010350357\nEpoch 4/20, Loss: 146.71465913749034\nEpoch 4/20, Loss: 156.36315691912617\nEpoch 4/20, Loss: 162.5179857795621\nEpoch 4/20, Loss: 166.55776977539062\nEpoch 4/20, Loss: 172.60164124288676\nEpoch 4/20, Loss: 189.2624229148582\nEpoch 4/20, Loss: 195.1532777385947\nEpoch 4/20, Loss: 203.2439059033806\nEpoch 4/20, Loss: 210.47172301492574\nEpoch 4/20, Loss: 223.17267335491417\nEpoch 4/20, Loss: 229.95328436957465\nEpoch 4/20, Loss: 236.54398261176215\nEpoch 4/20, Loss: 245.43064560124904\nEpoch 4/20, Loss: 253.760494656033\nEpoch 4/20, Loss: 264.7790063928675\nEpoch 4/20, Loss: 276.72799004448785\nEpoch 4/20, Loss: 282.6799018765673\nEpoch 4/20, Loss: 298.30669676227336\nEpoch 4/20, Loss: 304.41406023943864\nEpoch 4/20, Loss: 311.03049195842976\nEpoch 4/20, Loss: 317.48008219401044\nEpoch 4/20, Loss: 321.7875072337963\nEpoch 4/20, Loss: 333.80147976345484\nEpoch 4/20, Loss: 337.93552879050924\nEpoch 4/20, Loss: 350.94053819444446\nEpoch 4/20, Loss: 359.25014618296683\nEpoch 4/20, Loss: 364.0936008029514\nEpoch 4/20, Loss: 370.5108130184221\nEpoch 4/20, Loss: 377.3669923382041\nEpoch 4/20, Loss: 382.8710361056858\nEpoch 4/20, Loss: 392.31071679386093\nEpoch 4/20, Loss: 398.3408176751784\nEpoch 4/20, Loss: 404.88004218207465\nEpoch 4/20, Loss: 413.2991562831549\nEpoch 4/20, Loss: 424.6003798496576\nEpoch 4/20, Loss: 429.3442458164545\nEpoch 4/20, Loss: 432.972174750434\nEpoch 4/20, Loss: 438.9901835123698\nEpoch 4/20, Loss: 445.562122863016\nEpoch 4/20, Loss: 450.54748196072046\nEpoch 4/20, Loss: 458.0542086377556\nEpoch 4/20, Loss: 465.2811629683883\nEpoch 4/20, Loss: 477.603928177445\nEpoch 4/20, Loss: 485.5174368399161\nEpoch 4/20, Loss: 492.2160942171827\nEpoch 4/20, Loss: 496.3336769386574\nEpoch 4/20, Loss: 508.2472006715374\nEpoch 4/20, Loss: 515.0986644603588\nEpoch 4/20, Loss: 519.7209664803964\nEpoch 4/20, Loss: 524.7742064676168\nEpoch 4/20, Loss: 530.0693363142602\nEpoch 4/20, Loss: 542.372046576606\nEpoch 4/20, Loss: 547.3480778446904\nEpoch 4/20, Loss: 554.1338097843123\nEpoch 4/20, Loss: 559.6238565327209\nEpoch 4/20, Loss: 566.3939785427517\nEpoch 4/20, Loss: 570.0703848379629\nEpoch 4/20, Loss: 576.1756863064236\nEpoch 4/20, Loss: 580.6265013894918\nEpoch 4/20, Loss: 586.4457465277778\nEpoch 4/20, Loss: 593.2927042643229\nEpoch 4/20, Loss: 598.8527387454186\nEpoch 4/20, Loss: 602.7615394121335\nEpoch 4/20, Loss: 608.4105375313464\nEpoch 4/20, Loss: 615.1684909396702\nEpoch 4/20, Loss: 629.0658554265528\nEpoch 4/20, Loss: 635.8979635356385\nEpoch 4/20, Loss: 642.5252745828511\nEpoch 4/20, Loss: 646.503582989728\nEpoch 4/20, Loss: 661.061142909674\nEpoch 4/20, Loss: 669.3161455319251\nEpoch 4/20, Loss: 681.3868031442901\nEpoch 4/20, Loss: 689.5505333417727\nEpoch 4/20, Loss: 696.8947286723572\nEpoch 4/20, Loss: 703.4698516468943\nEpoch 4/20, Loss: 719.895292305652\nEpoch 4/20, Loss: 726.1873278205777\nEpoch 4/20, Loss: 732.4712498040847\nEpoch 4/20, Loss: 737.2124622486256\nEpoch 4/20, Loss: 743.4167540750386\nEpoch 4/20, Loss: 749.1224463192033\nEpoch 4/20, Loss: 752.7556770230517\nEpoch 4/20, Loss: 766.8068651740933\nEpoch 4/20, Loss: 773.5136492693865\nEpoch 4/20, Loss: 780.1026023582176\nEpoch 4/20, Loss: 787.0944854359568\nEpoch 4/20, Loss: 790.3457871425298\nEpoch 4/20, Loss: 796.2266367217641\nEpoch 4/20, Loss: 803.6590919023679\nEpoch 4/20, Loss: 808.5191948031202\nEpoch 4/20, Loss: 812.6883850097656\nEpoch 4/20, Loss: 818.1556667751736\nEpoch 4/20, Loss: 823.115209132065\nEpoch 4/20, Loss: 833.0422811625916\nEpoch 4/20, Loss: 844.7813189471209\nEpoch 4/20, Loss: 854.1575388967256\nEpoch 4/20, Loss: 868.5234514401283\nEpoch 4/20, Loss: 884.1267440230758\nEpoch 4/20, Loss: 887.7467802071277\nEpoch 4/20, Loss: 894.2456137574749\nEpoch 4/20, Loss: 900.4714023919753\nEpoch 4/20, Loss: 908.5432950243538\nEpoch 4/20, Loss: 914.7818633656443\nEpoch 4/20, Loss: 921.9138726128472\nEpoch 4/20, Loss: 931.2942776150173\nEpoch 4/20, Loss: 937.0116113733362\nEpoch 4/20, Loss: 942.0031836238908\nEpoch 4/20, Loss: 946.951431086034\nEpoch 4/20, Loss: 953.4497010030864\nEpoch 4/20, Loss: 960.7876172477817\nEpoch 4/20, Loss: 967.4792186595776\nEpoch 4/20, Loss: 971.7288897479021\nEpoch 4/20, Loss: 979.899467939212\nEpoch 4/20, Loss: 984.5490191424334\nEpoch 4/20, Loss: 988.6001745153357\nEpoch 4/20, Loss: 992.2412459762008\nEpoch 4/20, Loss: 999.4668778784481\nEpoch 4/20, Loss: 1005.407639114945\nEpoch 4/20, Loss: 1012.4283081808207\nEpoch 4/20, Loss: 1020.2413763352382\nEpoch 4/20, Loss: 1026.2721602828415\nEpoch 4/20, Loss: 1033.444589873891\nEpoch 4/20, Loss: 1044.7163123613523\nEpoch 4/20, Loss: 1051.3974737473477\nEpoch 4/20, Loss: 1057.8871309256847\nEpoch 4/20, Loss: 1065.1411170488523\nEpoch 4/20, Loss: 1073.9047987196182\nEpoch 4/20, Loss: 1078.498612768856\nEpoch 4/20, Loss: 1084.7207912868923\nEpoch 4/20, Loss: 1092.2599366741415\nEpoch 4/20, Loss: 1099.9588924455054\nEpoch 4/20, Loss: 1106.8525752314815\nEpoch 4/20, Loss: 1120.8508180217977\nEpoch 4/20, Loss: 1124.3626392505787\nEpoch 4/20, Loss: 1130.4120426884404\nEpoch 4/20, Loss: 1136.8753703553\nEpoch 4/20, Loss: 1143.7904493543838\nEpoch 4/20, Loss: 1150.893738169729\nEpoch 4/20, Loss: 1155.4178534613716\nEpoch 4/20, Loss: 1159.467273476683\nEpoch 4/20, Loss: 1165.9279623149355\nEpoch 4/20, Loss: 1174.1029139389227\nEpoch 4/20, Loss: 1182.2946110478154\nEpoch 4/20, Loss: 1186.9321511351031\nEpoch 5/20, Loss: 1186.9321511351031\nEpoch 5/20, Loss: 4.656522397641782\nEpoch 5/20, Loss: 8.426106770833334\nEpoch 5/20, Loss: 15.015940725067516\nEpoch 5/20, Loss: 26.401741687162424\nEpoch 5/20, Loss: 32.52515364281925\nEpoch 5/20, Loss: 43.65127940236786\nEpoch 5/20, Loss: 49.43064484772859\nEpoch 5/20, Loss: 58.64312706464602\nEpoch 5/20, Loss: 64.94221100983796\nEpoch 5/20, Loss: 75.5939180350598\nEpoch 5/20, Loss: 82.51654655550733\nEpoch 5/20, Loss: 87.94295774860147\nEpoch 5/20, Loss: 92.82093038676697\nEpoch 5/20, Loss: 100.7222742151331\nEpoch 5/20, Loss: 112.93318986304013\nEpoch 5/20, Loss: 123.04075942804783\nEpoch 5/20, Loss: 129.60502417293597\nEpoch 5/20, Loss: 135.83900997962482\nEpoch 5/20, Loss: 140.3645445270303\nEpoch 5/20, Loss: 146.69331604757426\nEpoch 5/20, Loss: 152.666801923587\nEpoch 5/20, Loss: 159.4588589138455\nEpoch 5/20, Loss: 167.31671255606193\nEpoch 5/20, Loss: 175.2648846661603\nEpoch 5/20, Loss: 185.69183990101754\nEpoch 5/20, Loss: 191.88938282154226\nEpoch 5/20, Loss: 196.68232670536747\nEpoch 5/20, Loss: 202.76856938114872\nEpoch 5/20, Loss: 209.10167891890913\nEpoch 5/20, Loss: 218.04217416268807\nEpoch 5/20, Loss: 225.14800912362557\nEpoch 5/20, Loss: 228.64331130039545\nEpoch 5/20, Loss: 236.8581904658565\nEpoch 5/20, Loss: 243.8209831331983\nEpoch 5/20, Loss: 250.627599645544\nEpoch 5/20, Loss: 257.6742478358893\nEpoch 5/20, Loss: 263.91522480529034\nEpoch 5/20, Loss: 277.3445747281298\nEpoch 5/20, Loss: 282.52413752049574\nEpoch 5/20, Loss: 289.5271048840181\nEpoch 5/20, Loss: 294.2771681797357\nEpoch 5/20, Loss: 299.4097493489583\nEpoch 5/20, Loss: 302.9990045994888\nEpoch 5/20, Loss: 306.52811987606094\nEpoch 5/20, Loss: 312.1632159197772\nEpoch 5/20, Loss: 325.95208099741996\nEpoch 5/20, Loss: 331.9207255045573\nEpoch 5/20, Loss: 338.1044631769628\nEpoch 5/20, Loss: 346.11998645170235\nEpoch 5/20, Loss: 357.3355400179639\nEpoch 5/20, Loss: 362.1692738474151\nEpoch 5/20, Loss: 373.8527884777681\nEpoch 5/20, Loss: 379.8507083845727\nEpoch 5/20, Loss: 386.1492916154273\nEpoch 5/20, Loss: 394.6865614902826\nEpoch 5/20, Loss: 403.37683670609084\nEpoch 5/20, Loss: 417.7760126561294\nEpoch 5/20, Loss: 423.1614673755787\nEpoch 5/20, Loss: 429.01918952847706\nEpoch 5/20, Loss: 436.58453858928914\nEpoch 5/20, Loss: 444.0601712450569\nEpoch 5/20, Loss: 451.0190885567371\nEpoch 5/20, Loss: 456.27952481493537\nEpoch 5/20, Loss: 471.56932727201485\nEpoch 5/20, Loss: 477.2775641547309\nEpoch 5/20, Loss: 482.7844728069541\nEpoch 5/20, Loss: 488.8275790744358\nEpoch 5/20, Loss: 492.41422488365646\nEpoch 5/20, Loss: 496.73793161651236\nEpoch 5/20, Loss: 502.57161910445603\nEpoch 5/20, Loss: 509.95179352936924\nEpoch 5/20, Loss: 515.3977815604504\nEpoch 5/20, Loss: 522.9604556236739\nEpoch 5/20, Loss: 537.975000075352\nEpoch 5/20, Loss: 545.1523245352286\nEpoch 5/20, Loss: 551.3443705240885\nEpoch 5/20, Loss: 556.0209930796682\nEpoch 5/20, Loss: 562.5881731951678\nEpoch 5/20, Loss: 570.5203993055555\nEpoch 5/20, Loss: 577.1254272460938\nEpoch 5/20, Loss: 584.1258899076486\nEpoch 5/20, Loss: 591.0812792365933\nEpoch 5/20, Loss: 598.2753800757138\nEpoch 5/20, Loss: 602.8496689031153\nEpoch 5/20, Loss: 608.2198678475839\nEpoch 5/20, Loss: 614.5603272237895\nEpoch 5/20, Loss: 620.7127745828511\nEpoch 5/20, Loss: 632.0062157901717\nEpoch 5/20, Loss: 638.5908934039834\nEpoch 5/20, Loss: 644.8782171555507\nEpoch 5/20, Loss: 650.1245245285976\nEpoch 5/20, Loss: 654.2042040412808\nEpoch 5/20, Loss: 660.7818837106963\nEpoch 5/20, Loss: 666.4122159981433\nEpoch 5/20, Loss: 670.6768406997493\nEpoch 5/20, Loss: 678.0132506570699\nEpoch 5/20, Loss: 684.0187799901138\nEpoch 5/20, Loss: 691.5008288724923\nEpoch 5/20, Loss: 698.3335910373264\nEpoch 5/20, Loss: 702.0626382710021\nEpoch 5/20, Loss: 705.3780705958237\nEpoch 5/20, Loss: 709.1517398033614\nEpoch 5/20, Loss: 724.4502378864053\nEpoch 5/20, Loss: 737.8675751862703\nEpoch 5/20, Loss: 745.3759204252267\nEpoch 5/20, Loss: 751.4298419716918\nEpoch 5/20, Loss: 756.0841377164111\nEpoch 5/20, Loss: 766.8908227991175\nEpoch 5/20, Loss: 770.89437979239\nEpoch 5/20, Loss: 775.4489222397038\nEpoch 5/20, Loss: 780.9403573495371\nEpoch 5/20, Loss: 787.5045821578415\nEpoch 5/20, Loss: 791.3595452956212\nEpoch 5/20, Loss: 799.0744282286844\nEpoch 5/20, Loss: 803.0092212064767\nEpoch 5/20, Loss: 807.8749596866561\nEpoch 5/20, Loss: 811.2255780255354\nEpoch 5/20, Loss: 815.3846017343027\nEpoch 5/20, Loss: 826.5276229293258\nEpoch 5/20, Loss: 837.3058415165654\nEpoch 5/20, Loss: 842.8665925956067\nEpoch 5/20, Loss: 848.8932114589361\nEpoch 5/20, Loss: 851.9073778317298\nEpoch 5/20, Loss: 857.8708335970655\nEpoch 5/20, Loss: 868.8954797438633\nEpoch 5/20, Loss: 873.1682673324773\nEpoch 5/20, Loss: 878.4864266477986\nEpoch 5/20, Loss: 884.1115547462746\nEpoch 5/20, Loss: 889.3816824077088\nEpoch 5/20, Loss: 893.083565229251\nEpoch 5/20, Loss: 898.7132687038845\nEpoch 5/20, Loss: 904.3947051248433\nEpoch 5/20, Loss: 908.7274703037592\nEpoch 5/20, Loss: 913.6171778926143\nEpoch 5/20, Loss: 919.8138463526597\nEpoch 5/20, Loss: 932.9164773446543\nEpoch 5/20, Loss: 936.6964037859881\nEpoch 5/20, Loss: 941.9760344705464\nEpoch 5/20, Loss: 946.5550599274812\nEpoch 5/20, Loss: 952.3114068302108\nEpoch 5/20, Loss: 958.5464193085094\nEpoch 5/20, Loss: 961.8353944001375\nEpoch 5/20, Loss: 968.0183902316624\nEpoch 5/20, Loss: 973.7044291555146\nEpoch 5/20, Loss: 977.2839029571156\nEpoch 5/20, Loss: 981.1146641484013\nEpoch 5/20, Loss: 986.5271325758946\nEpoch 5/20, Loss: 990.7670286202136\nEpoch 5/20, Loss: 1001.9365970941237\nEpoch 5/20, Loss: 1005.2719965805242\nEpoch 5/20, Loss: 1013.8869792796947\nEpoch 5/20, Loss: 1024.67983876923\nEpoch 5/20, Loss: 1028.761181678301\nEpoch 5/20, Loss: 1036.2100440131294\nEpoch 5/20, Loss: 1040.0115094596956\nEpoch 5/20, Loss: 1046.032235039605\nEpoch 5/20, Loss: 1053.0207664583936\nEpoch 5/20, Loss: 1067.335659639335\nEpoch 5/20, Loss: 1076.391093124578\nEpoch 5/20, Loss: 1080.7169123520086\nEpoch 5/20, Loss: 1093.1728434621552\nEpoch 5/20, Loss: 1099.193644017349\nEpoch 6/20, Loss: 1099.193644017349\nEpoch 6/20, Loss: 5.638381016107253\nEpoch 6/20, Loss: 11.611527054398149\nEpoch 6/20, Loss: 19.073967074170525\nEpoch 6/20, Loss: 26.193372938368057\nEpoch 6/20, Loss: 33.86291202498071\nEpoch 6/20, Loss: 37.94179129894869\nEpoch 6/20, Loss: 42.214662905092595\nEpoch 6/20, Loss: 52.76094714506173\nEpoch 6/20, Loss: 56.526816662446954\nEpoch 6/20, Loss: 61.07452392578125\nEpoch 6/20, Loss: 66.97129689911266\nEpoch 6/20, Loss: 72.97940741644965\nEpoch 6/20, Loss: 78.490044111087\nEpoch 6/20, Loss: 84.36229074737172\nEpoch 6/20, Loss: 89.13866660035687\nEpoch 6/20, Loss: 94.49475662796586\nEpoch 6/20, Loss: 102.67110829294464\nEpoch 6/20, Loss: 110.08235639407312\nEpoch 6/20, Loss: 120.62642151632427\nEpoch 6/20, Loss: 123.89335199049961\nEpoch 6/20, Loss: 137.51265537591627\nEpoch 6/20, Loss: 141.03301060052564\nEpoch 6/20, Loss: 145.43204564224055\nEpoch 6/20, Loss: 155.5980160560137\nEpoch 6/20, Loss: 159.48803673261477\nEpoch 6/20, Loss: 164.84198751567322\nEpoch 6/20, Loss: 168.07500090422454\nEpoch 6/20, Loss: 173.26581564067322\nEpoch 6/20, Loss: 178.57841887297454\nEpoch 6/20, Loss: 184.37642415364584\nEpoch 6/20, Loss: 190.17019201208043\nEpoch 6/20, Loss: 194.33251274956598\nEpoch 6/20, Loss: 199.85508709189332\nEpoch 6/20, Loss: 206.19611537603686\nEpoch 6/20, Loss: 210.59868819625288\nEpoch 6/20, Loss: 221.24837201907312\nEpoch 6/20, Loss: 232.1931231463397\nEpoch 6/20, Loss: 243.07078005943768\nEpoch 6/20, Loss: 249.06097977249712\nEpoch 6/20, Loss: 252.79049155152873\nEpoch 6/20, Loss: 263.1839912320361\nEpoch 6/20, Loss: 268.71837248625576\nEpoch 6/20, Loss: 275.93759117597415\nEpoch 6/20, Loss: 281.52827773859474\nEpoch 6/20, Loss: 285.93528088228203\nEpoch 6/20, Loss: 296.5813478069541\nEpoch 6/20, Loss: 303.1414666823399\nEpoch 6/20, Loss: 313.60471975067514\nEpoch 6/20, Loss: 319.2435574001736\nEpoch 6/20, Loss: 325.44910798249424\nEpoch 6/20, Loss: 330.23282387815874\nEpoch 6/20, Loss: 339.8173681188513\nEpoch 6/20, Loss: 347.23476796091336\nEpoch 6/20, Loss: 350.4965925805363\nEpoch 6/20, Loss: 356.2531289936584\nEpoch 6/20, Loss: 366.0270916974103\nEpoch 6/20, Loss: 371.39347481433254\nEpoch 6/20, Loss: 374.7598846812307\nEpoch 6/20, Loss: 380.3057469497492\nEpoch 6/20, Loss: 386.946770562066\nEpoch 6/20, Loss: 395.3209160698785\nEpoch 6/20, Loss: 399.89630390685284\nEpoch 6/20, Loss: 406.43837219991804\nEpoch 6/20, Loss: 410.9292255919657\nEpoch 6/20, Loss: 415.9230007595486\nEpoch 6/20, Loss: 421.59418666509936\nEpoch 6/20, Loss: 426.63363233024694\nEpoch 6/20, Loss: 439.3075267650463\nEpoch 6/20, Loss: 445.4318843888648\nEpoch 6/20, Loss: 448.67318537205824\nEpoch 6/20, Loss: 457.4313505196277\nEpoch 6/20, Loss: 460.71901410891684\nEpoch 6/20, Loss: 465.5854567539545\nEpoch 6/20, Loss: 469.96276252652393\nEpoch 6/20, Loss: 477.18115837191357\nEpoch 6/20, Loss: 483.84612886405284\nEpoch 6/20, Loss: 490.1126897364487\nEpoch 6/20, Loss: 495.75524713963637\nEpoch 6/20, Loss: 509.5633737069589\nEpoch 6/20, Loss: 514.6390678499952\nEpoch 6/20, Loss: 522.0001827287085\nEpoch 6/20, Loss: 527.4277501989294\nEpoch 6/20, Loss: 533.3730860580632\nEpoch 6/20, Loss: 539.3016850977768\nEpoch 6/20, Loss: 543.141088038315\nEpoch 6/20, Loss: 549.7941174919223\nEpoch 6/20, Loss: 553.763643241223\nEpoch 6/20, Loss: 559.7558213222173\nEpoch 6/20, Loss: 564.6942877121913\nEpoch 6/20, Loss: 568.3822869194878\nEpoch 6/20, Loss: 578.86496499144\nEpoch 6/20, Loss: 589.0007765028212\nEpoch 6/20, Loss: 603.3635061758536\nEpoch 6/20, Loss: 609.9138255178193\nEpoch 6/20, Loss: 622.0361927173756\nEpoch 6/20, Loss: 627.5393092779466\nEpoch 6/20, Loss: 634.4624969105662\nEpoch 6/20, Loss: 639.9852694287712\nEpoch 6/20, Loss: 644.1095272111304\nEpoch 6/20, Loss: 649.5882884837963\nEpoch 6/20, Loss: 652.7299151008511\nEpoch 6/20, Loss: 656.1513372350622\nEpoch 6/20, Loss: 662.616529111509\nEpoch 6/20, Loss: 668.120029590748\nEpoch 6/20, Loss: 672.6656522397642\nEpoch 6/20, Loss: 680.8699587598259\nEpoch 6/20, Loss: 689.0293117570288\nEpoch 6/20, Loss: 694.1815330599561\nEpoch 6/20, Loss: 699.8779705659842\nEpoch 6/20, Loss: 703.534905516071\nEpoch 6/20, Loss: 710.9240076512466\nEpoch 6/20, Loss: 717.387064050745\nEpoch 6/20, Loss: 723.1270324330271\nEpoch 6/20, Loss: 728.364249335395\nEpoch 6/20, Loss: 733.8004928400487\nEpoch 6/20, Loss: 739.7054222954644\nEpoch 6/20, Loss: 745.8352429425275\nEpoch 6/20, Loss: 753.158140771183\nEpoch 6/20, Loss: 758.5412855737003\nEpoch 6/20, Loss: 763.9874359884379\nEpoch 6/20, Loss: 771.0274953959901\nEpoch 6/20, Loss: 777.116596551589\nEpoch 6/20, Loss: 781.8541370909891\nEpoch 6/20, Loss: 787.4885975402078\nEpoch 6/20, Loss: 799.7505355646581\nEpoch 6/20, Loss: 805.6146882610557\nEpoch 6/20, Loss: 812.2896187864704\nEpoch 6/20, Loss: 824.0196391918041\nEpoch 6/20, Loss: 837.6384859438296\nEpoch 6/20, Loss: 879.8800870165413\nEpoch 6/20, Loss: 887.5015417028357\nEpoch 6/20, Loss: 891.4642081555025\nEpoch 6/20, Loss: 895.0110424653983\nEpoch 6/20, Loss: 907.2595957061391\nEpoch 6/20, Loss: 916.2071092393663\nEpoch 6/20, Loss: 921.1775893223139\nEpoch 6/20, Loss: 924.6304106535734\nEpoch 6/20, Loss: 931.457415168668\nEpoch 6/20, Loss: 936.6318939585744\nEpoch 6/20, Loss: 942.5176613890095\nEpoch 6/20, Loss: 948.0691686559607\nEpoch 6/20, Loss: 951.8820269549334\nEpoch 6/20, Loss: 965.6280890570747\nEpoch 6/20, Loss: 969.8879692171827\nEpoch 6/20, Loss: 975.7207649136767\nEpoch 6/20, Loss: 980.0264836064091\nEpoch 6/20, Loss: 986.0832304777922\nEpoch 6/20, Loss: 989.1001888322241\nEpoch 6/20, Loss: 993.904296875\nEpoch 6/20, Loss: 998.4915447470582\nEpoch 6/20, Loss: 1001.8072483392409\nEpoch 6/20, Loss: 1007.7132892373168\nEpoch 6/20, Loss: 1013.8011316370081\nEpoch 6/20, Loss: 1017.4598637333622\nEpoch 6/20, Loss: 1020.8832777988764\nEpoch 7/20, Loss: 1020.8832777988764\nEpoch 7/20, Loss: 4.736598638840664\nEpoch 7/20, Loss: 13.609169288917824\nEpoch 7/20, Loss: 20.657967273099924\nEpoch 7/20, Loss: 24.831516972294562\nEpoch 7/20, Loss: 30.50872576678241\nEpoch 7/20, Loss: 33.585724818853684\nEpoch 7/20, Loss: 37.27409513497058\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Evaluating the model\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(X_test_tensor)\n    test_loss = criterion(predictions, y_test_tensor)\n    print(f'Test Loss: {test_loss.item()}')\n\n# Visualize some predictions\nimport matplotlib.pyplot as plt\n\nplt.plot(y_test_tensor.numpy()[0][:, 0], label='True Angles')\nplt.plot(predictions.numpy()[0][:, 0], label='Predicted Angles')\nplt.legend()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  }
 ]
}
